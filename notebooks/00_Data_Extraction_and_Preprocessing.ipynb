{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Extraction and Preprocessing\n"
      ],
      "metadata": {
        "id": "U7NXaaYArTSD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Demographics"
      ],
      "metadata": {
        "id": "cKf9upHourke"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extracting Demographic information"
      ],
      "metadata": {
        "id": "nPa0F9SFrZ6H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBmhIlq7rBwq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Set base path to WESAD dataset folder\n",
        "base_path = \"/content/drive/MyDrive/Capstone/WESAD\"\n",
        "output_file = \"/content/drive/MyDrive/Capstone/demographic_info.csv\"\n",
        "\n",
        "# Initialize a list to store patient demographic data\n",
        "patient_data = []\n",
        "\n",
        "# Function to process a readme file\n",
        "def process_readme_file(readme_path):\n",
        "    \"\"\"\n",
        "    Extract demographic information from the readme file.\n",
        "\n",
        "    Parameters:\n",
        "        readme_path (str): Path to the readme file.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the extracted demographic data.\n",
        "    \"\"\"\n",
        "    demographics = {}\n",
        "    try:\n",
        "        with open(readme_path, 'r') as file:\n",
        "            lines = file.readlines()\n",
        "            for line in lines:\n",
        "                line = line.strip()\n",
        "                # Skip empty lines or lines without \":\"\n",
        "                if not line or \":\" not in line:\n",
        "                    continue\n",
        "\n",
        "                key, value = map(str.strip, line.split(\":\", 1))  # Split on the first \":\"\n",
        "\n",
        "                # Map keys to corresponding fields\n",
        "                if key == \"Age\":\n",
        "                    demographics[\"Age\"] = int(value)\n",
        "                elif key == \"Height (cm)\":\n",
        "                    demographics[\"Height_cm\"] = int(value)\n",
        "                elif key == \"Weight (kg)\":\n",
        "                    demographics[\"Weight_kg\"] = int(value)\n",
        "                elif key == \"Gender\":\n",
        "                    demographics[\"Gender\"] = value\n",
        "                elif key == \"Dominant hand\":\n",
        "                    demographics[\"Dominant_Hand\"] = value\n",
        "                elif key == \"Did you drink coffee today?\":\n",
        "                    demographics[\"Coffee_Today\"] = value\n",
        "                elif key == \"Did you drink coffee within the last hour?\":\n",
        "                    demographics[\"Coffee_Last_Hour\"] = value\n",
        "                elif key == \"Did you do any sports today?\":\n",
        "                    demographics[\"Sports_Today\"] = value\n",
        "                elif key == \"Are you a smoker?\":\n",
        "                    demographics[\"Smoker\"] = value\n",
        "                elif key == \"Did you smoke within the last hour?\":\n",
        "                    demographics[\"Smoke_Last_Hour\"] = value\n",
        "                elif key == \"Do you feel ill today?\":\n",
        "                    demographics[\"Feel_Ill\"] = value\n",
        "\n",
        "                # Handle additional notes\n",
        "                elif key == \"### Additional notes ###\":\n",
        "                    # Collect additional notes from the subsequent lines\n",
        "                    notes_index = lines.index(line) + 1\n",
        "                    additional_notes = \" \".join([l.strip() for l in lines[notes_index:]])\n",
        "                    demographics[\"Additional_Notes\"] = additional_notes\n",
        "                    break  # Stop processing further lines\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {readme_path}: {e}\")\n",
        "    return demographics\n",
        "\n",
        "\n",
        "# Process each patient's readme file\n",
        "for patient_folder in [os.path.join(base_path, f) for f in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, f))]:\n",
        "    patient_id = os.path.basename(patient_folder)\n",
        "    readme_path = os.path.join(patient_folder, f\"{patient_id}_readme.txt\")\n",
        "\n",
        "    if os.path.exists(readme_path):\n",
        "        print(f\"Processing {readme_path}\")\n",
        "        demographics = process_readme_file(readme_path)\n",
        "        demographics[\"Patient_ID\"] = patient_id  # Add patient ID\n",
        "        patient_data.append(demographics)\n",
        "    else:\n",
        "        print(f\"Readme file not found for {patient_id}\")\n",
        "\n",
        "# Create a DataFrame from the collected data\n",
        "demographics_df = pd.DataFrame(patient_data)\n",
        "\n",
        "# Save DataFrame to CSV\n",
        "demographics_df.to_csv(output_file, index=False)\n",
        "print(f\"Demographic information saved to {output_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Protocols"
      ],
      "metadata": {
        "id": "HJx-lksJulAG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract protocols"
      ],
      "metadata": {
        "id": "f0Y-atSPrgCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Set paths\n",
        "output_folder_quest = \"/content/drive/MyDrive/Capstone/output_quest_data\"  # Path to save questionnaire data\n",
        "os.makedirs(output_folder_quest, exist_ok=True)\n",
        "\n",
        "# Define required conditions\n",
        "REQUIRED_CONDITIONS = {\"Base\", \"TSST\", \"Medi 1\", \"Fun\", \"Medi 2\"}\n",
        "\n",
        "def extract_protocol_data(quest_path, patient_id):\n",
        "    \"\"\"\n",
        "    Extracts study protocol conditions and start/end times from the patient's quest file.\n",
        "    Filters only the required conditions: Base, TSST, Medi 1, Fun, and Medi 2.\n",
        "    Saves the data as a CSV file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Read the questionnaire file\n",
        "        df = pd.read_csv(quest_path, header=None, sep=\";\", skip_blank_lines=False)\n",
        "\n",
        "        # Initialize processed data dictionary\n",
        "        processed_data = {\"Condition\": [], \"Start_Time\": [], \"End_Time\": []}\n",
        "\n",
        "        # Extract Conditions\n",
        "        conditions_row = df[df[0].str.contains(\"ORDER\", na=False)]\n",
        "        if conditions_row.empty:\n",
        "            print(f\"Warning: Missing conditions row for {patient_id}\")\n",
        "            return\n",
        "\n",
        "        conditions = conditions_row.iloc[0, 1:].dropna().tolist()\n",
        "\n",
        "        # Extract START and END times\n",
        "        start_times_row = df[df[0].str.contains(\"START\", na=False)]\n",
        "        end_times_row = df[df[0].str.contains(\"END\", na=False)]\n",
        "\n",
        "        if start_times_row.empty or end_times_row.empty:\n",
        "            print(f\"Warning: Missing start/end times for {patient_id}\")\n",
        "            return\n",
        "\n",
        "        start_times = start_times_row.iloc[0, 1:].dropna().tolist()\n",
        "        end_times = end_times_row.iloc[0, 1:].dropna().tolist()\n",
        "\n",
        "        # Ensure alignment between conditions and times\n",
        "        min_length = min(len(conditions), len(start_times), len(end_times))\n",
        "        conditions = conditions[:min_length]\n",
        "        start_times = start_times[:min_length]\n",
        "        end_times = end_times[:min_length]\n",
        "\n",
        "        # Filter only required conditions\n",
        "        for cond, start, end in zip(conditions, start_times, end_times):\n",
        "            if cond in REQUIRED_CONDITIONS:\n",
        "                try:\n",
        "                    if start.isdigit():\n",
        "                        start_sec = float(start) * 60\n",
        "                    elif \".\" in start:\n",
        "                        start_sec = float(start.split(\".\")[0]) * 60 + float(start.split(\".\")[1])\n",
        "                    else:\n",
        "                        start_sec = None\n",
        "\n",
        "                    if end.isdigit():\n",
        "                        end_sec = float(end) * 60\n",
        "                    elif \".\" in end:\n",
        "                        end_sec = float(end.split(\".\")[0]) * 60 + float(end.split(\".\")[1])\n",
        "                    else:\n",
        "                        end_sec = None\n",
        "\n",
        "                    processed_data[\"Condition\"].append(cond)\n",
        "                    processed_data[\"Start_Time\"].append(start_sec)\n",
        "                    processed_data[\"End_Time\"].append(end_sec)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error converting times for {patient_id} in condition {cond}: {e}\")\n",
        "\n",
        "        # Create DataFrame and save\n",
        "        protocol_df = pd.DataFrame(processed_data)\n",
        "        output_file = os.path.join(output_folder_quest, f\"{patient_id}_protocol.csv\")\n",
        "        protocol_df.to_csv(output_file, index=False)\n",
        "        print(f\"Protocol data saved for {patient_id} to {output_file}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing protocol data for {patient_id}: {e}\")\n",
        "\n",
        "# Function to process all patients\n",
        "def process_protocols_all_patients(base_path):\n",
        "    \"\"\"\n",
        "    Iterate through all patient folders and extract protocol data.\n",
        "    \"\"\"\n",
        "    for patient_folder in [os.path.join(base_path, f) for f in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, f))]:\n",
        "        patient_id = os.path.basename(patient_folder)\n",
        "        quest_path = os.path.join(patient_folder, f\"{patient_id}_quest.csv\")\n",
        "        if os.path.exists(quest_path):\n",
        "            extract_protocol_data(quest_path, patient_id)\n",
        "\n",
        "# Example Usage:\n",
        "# Set base path where patient folders are located\n",
        "base_path = \"/content/drive/MyDrive/Capstone/WESAD\"\n",
        "\n",
        "# Run the script for all patients\n",
        "process_protocols_all_patients(base_path)\n"
      ],
      "metadata": {
        "id": "Y60uVGCpr2QA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Complied protocols"
      ],
      "metadata": {
        "id": "vszY-Cz-r4zw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the output folder where protocol files are stored\n",
        "output_folder_quest = \"/content/drive/MyDrive/Capstone/output_quest_data\"\n",
        "\n",
        "# List all protocol files\n",
        "protocol_files = [f for f in os.listdir(output_folder_quest) if f.endswith(\"_protocol.csv\")]\n",
        "\n",
        "# Initialize an empty list to store data from all patients\n",
        "all_data = []\n",
        "\n",
        "# Loop through each file and compile data\n",
        "for file in protocol_files:\n",
        "    patient_id = file.split(\"_protocol.csv\")[0]  # Extract patient ID from file name\n",
        "    file_path = os.path.join(output_folder_quest, file)\n",
        "\n",
        "    # Read the patient's protocol file\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Add patient ID column\n",
        "    df.insert(0, \"Patient_ID\", patient_id)\n",
        "\n",
        "    # Append to the list\n",
        "    all_data.append(df)\n",
        "\n",
        "# Combine all patient data into a single DataFrame\n",
        "compiled_df = pd.concat(all_data, ignore_index=True)\n",
        "\n",
        "#Add column for test duration\n",
        "compiled_df['Duration'] = compiled_df['End_Time'] - compiled_df['Start_Time']\n",
        "\n",
        "# Define output path for the compiled CSV file\n",
        "compiled_file_path = os.path.join(output_folder_quest, \"compiled_protocol_data.csv\")\n",
        "\n",
        "# Save the combined data to a CSV file\n",
        "compiled_df.to_csv(compiled_file_path, index=False)\n"
      ],
      "metadata": {
        "id": "5qZhTjlpr8t0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questionnaire"
      ],
      "metadata": {
        "id": "9BiZf1x6swWn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract questionnaire data"
      ],
      "metadata": {
        "id": "ZM7kFY-7sY82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Set paths\n",
        "output_folder_quest = \"/content/drive/MyDrive/Capstone/output_quest_data\"  # Path to save questionnaire data\n",
        "os.makedirs(output_folder_quest, exist_ok=True)\n",
        "\n",
        "def extract_survey_data(quest_path, patient_id):\n",
        "    \"\"\"\n",
        "    Extract PANAS, STAI, DIM, and SSSQ questionnaire responses from the patient's quest file.\n",
        "    Saves each survey as a separate CSV file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Read the questionnaire file\n",
        "        df = pd.read_csv(quest_path, header=None, sep=\";\", skip_blank_lines=False)\n",
        "\n",
        "        # Define survey types and their corresponding columns\n",
        "        survey_columns = {\n",
        "            \"PANAS\": [\n",
        "                \"PANAS_Active\", \"PANAS_Distressed\", \"PANAS_Interested\", \"PANAS_Inspired\",\n",
        "                \"PANAS_Annoyed\", \"PANAS_Strong\", \"PANAS_Guilty\", \"PANAS_Scared\",\n",
        "                \"PANAS_Hostile\", \"PANAS_Excited\", \"PANAS_Proud\", \"PANAS_Irritable\",\n",
        "                \"PANAS_Enthusiastic\", \"PANAS_Ashamed\", \"PANAS_Alert\", \"PANAS_Nervous\",\n",
        "                \"PANAS_Determined\", \"PANAS_Attentive\", \"PANAS_Jittery\", \"PANAS_Afraid\",\n",
        "                \"PANAS_Stressed\", \"PANAS_Frustrated\", \"PANAS_Happy\", \"PANAS_Angry\",\n",
        "                \"PANAS_Irritated\", \"PANAS_Sad\"\n",
        "            ],\n",
        "            \"STAI\": [\n",
        "                \"STAI_ease\", \"STAI_nervous\", \"STAI_jittery\", \"STAI_relaxed\",\n",
        "                \"STAI_worried\", \"STAI_pleasant\"\n",
        "            ],\n",
        "            \"DIM\": [\"SAM_valence\", \"SAM_arousal\"]\n",
        "          }\n",
        "\n",
        "        # Define conditions\n",
        "        conditions = [\"Base\", \"TSST\", \"Medi 1\", \"Fun\", \"Medi 2\"]\n",
        "\n",
        "        # Extract and save each survey separately\n",
        "        for survey, columns in survey_columns.items():\n",
        "            survey_rows = df[df[0].str.contains(survey, na=False)]\n",
        "            if not survey_rows.empty:\n",
        "                survey_data = survey_rows.iloc[:, 1:].apply(pd.to_numeric, errors='coerce')  # Convert to numeric\n",
        "\n",
        "                if survey == \"PANAS\":\n",
        "                  survey_data.columns = columns[:survey_data.shape[1]]  # Assign columns based on survey type\n",
        "\n",
        "                elif survey == \"STAI\":\n",
        "                  survey_data = survey_data.iloc[:, :6]  # Keep only the first 6 columns\n",
        "                  survey_data.columns = survey_columns[\"STAI\"]\n",
        "\n",
        "                elif survey == \"DIM\":\n",
        "                  survey_data = survey_data.iloc[:, :2]  # Keep only the first 2 columns\n",
        "                  survey_data.columns = survey_columns[\"DIM\"]\n",
        "\n",
        "\n",
        "                # Add CONDITION column\n",
        "                survey_data[\"CONDITION\"] = conditions[:len(survey_data)]\n",
        "\n",
        "                # Special handling for PANAS_Angry and PANAS_Irritated (only filled in TSST condition)\n",
        "                if survey == \"PANAS\":\n",
        "                    survey_data.loc[survey_data[\"CONDITION\"] != \"TSST\", [\"PANAS_Sad\"]] = survey_data['PANAS_Angry']\n",
        "                    survey_data.loc[survey_data[\"CONDITION\"] != \"TSST\", [\"PANAS_Angry\", \"PANAS_Irritated\"]] = float('NaN')\n",
        "\n",
        "                # Save to CSV\n",
        "                output_file = os.path.join(output_folder_quest, f\"{patient_id}_{survey}.csv\")\n",
        "                survey_data.to_csv(output_file, index=False)\n",
        "                print(f\"Saved {survey} data for {patient_id} to {output_file}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing questionnaire data for {patient_id}: {e}\")\n",
        "\n",
        "# Function to process all patients\n",
        "def process_survey_data_all_patients(base_path):\n",
        "    \"\"\"\n",
        "    Iterate through all patient folders and extract questionnaire data.\n",
        "    \"\"\"\n",
        "    for patient_folder in [os.path.join(base_path, f) for f in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, f))]:\n",
        "        patient_id = os.path.basename(patient_folder)\n",
        "        quest_path = os.path.join(patient_folder, f\"{patient_id}_quest.csv\")\n",
        "        if os.path.exists(quest_path):\n",
        "            extract_survey_data(quest_path, patient_id)\n",
        "\n",
        "# Example Usage:\n",
        "# Set base path where patient folders are located\n",
        "base_path = \"/content/drive/MyDrive/Capstone/WESAD\"\n",
        "\n",
        "# Run the script for all patients\n",
        "process_survey_data_all_patients(base_path)"
      ],
      "metadata": {
        "id": "rF7WknRjseAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combine questionnaire data for each patient"
      ],
      "metadata": {
        "id": "_3hamaZ8sr7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Define the folder paths where the extracted survey data is saved\n",
        "output_folder_quest = \"/content/drive/MyDrive/Capstone/output_quest_data\"\n",
        "output_folder_merged = \"/content/drive/MyDrive/Capstone/output_merged_quest_data\"\n",
        "os.makedirs(output_folder_merged, exist_ok=True)\n",
        "\n",
        "def merge_surveys(patient_id):\n",
        "    \"\"\"\n",
        "    Merge PANAS, STAI, and DIM CSV files into one CSV for each patient.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Define paths to the individual survey and protocol files\n",
        "        panas_file = os.path.join(output_folder_quest, f\"{patient_id}_PANAS.csv\")\n",
        "        stai_file = os.path.join(output_folder_quest, f\"{patient_id}_STAI.csv\")\n",
        "        dim_file = os.path.join(output_folder_quest, f\"{patient_id}_DIM.csv\")\n",
        "        protocol_file = os.path.join(output_folder_quest, f\"{patient_id}_protocol.csv\")\n",
        "\n",
        "        # Check if all files exist before proceeding\n",
        "        if os.path.exists(panas_file) and os.path.exists(stai_file) and os.path.exists(dim_file):\n",
        "            # Read the survey CSV files\n",
        "            panas_df = pd.read_csv(panas_file)\n",
        "            stai_df = pd.read_csv(stai_file)\n",
        "            dim_df = pd.read_csv(dim_file)\n",
        "            protocol_df = pd.read_csv(protocol_file)\n",
        "\n",
        "            # Rename 'Condition' in protocol to match 'CONDITION' in other files\n",
        "            protocol_df.rename(columns={'Condition': 'CONDITION'}, inplace=True)\n",
        "\n",
        "            # Merge the dataframes on the 'CONDITION' column\n",
        "            merged_df = panas_df.merge(stai_df, on='CONDITION').merge(dim_df, on='CONDITION').merge(protocol_df, on='CONDITION')\n",
        "\n",
        "            # Reorder columns to ensure CONDITION, Start_Time, and End_Time are first\n",
        "            cols = merged_df.columns.tolist()\n",
        "            cols.insert(0, cols.pop(cols.index('CONDITION')))\n",
        "            cols.insert(1, cols.pop(cols.index('Start_Time')))\n",
        "            cols.insert(2, cols.pop(cols.index('End_Time')))\n",
        "            merged_df = merged_df[cols]\n",
        "\n",
        "            # Save the merged dataframe to a new CSV file\n",
        "            merged_output_file = os.path.join(output_folder_merged, f\"{patient_id}_merged.csv\")\n",
        "            merged_df.to_csv(merged_output_file, index=False)\n",
        "            print(f\"Merged survey data saved for {patient_id} to {merged_output_file}\")\n",
        "        else:\n",
        "            print(f\"Missing one or more survey files for {patient_id}. Skipping merge.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error merging survey data for {patient_id}: {e}\")\n",
        "\n",
        "# Function to process and merge data for all patients\n",
        "def process_and_merge_all_patients(base_path):\n",
        "    \"\"\"\n",
        "    Extract questionnaire data and merge PANAS, STAI, and DIM for all patients.\n",
        "    \"\"\"\n",
        "    for patient_folder in [os.path.join(base_path, f) for f in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, f))]:\n",
        "        patient_id = os.path.basename(patient_folder)\n",
        "        merge_surveys(patient_id)\n",
        "\n",
        "# Example usage\n",
        "base_path = \"/content/drive/MyDrive/Capstone/WESAD\"\n",
        "process_and_merge_all_patients(base_path)\n"
      ],
      "metadata": {
        "id": "YQCelxwsshOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compile questionnaire data for all patients"
      ],
      "metadata": {
        "id": "2UJtFHPit_AI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Define folder paths\n",
        "output_folder_quest = \"/content/drive/MyDrive/Capstone/output_quest_data\"\n",
        "protocol_file = \"/content/drive/MyDrive/Capstone/output_quest_data/compiled_protocol_data.csv\"\n",
        "output_merged_file = \"/content/drive/MyDrive/Capstone/compiled_survey_data.csv\"\n",
        "\n",
        "# Load protocol data\n",
        "protocol_data = pd.read_csv(protocol_file)\n",
        "\n",
        "# Initialize list to collect all merged survey data\n",
        "all_survey_data = []\n",
        "\n",
        "# Function to merge surveys for a single patient\n",
        "def merge_surveys(patient_id, output_folder_quest, protocol_data):\n",
        "    \"\"\"\n",
        "    Merge PANAS, STAI, and DIM survey data into a single dataset for a patient.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Define paths to individual survey files\n",
        "        panas_file = os.path.join(output_folder_quest, f\"{patient_id}_PANAS.csv\")\n",
        "        stai_file = os.path.join(output_folder_quest, f\"{patient_id}_STAI.csv\")\n",
        "        dim_file = os.path.join(output_folder_quest, f\"{patient_id}_DIM.csv\")\n",
        "\n",
        "        # Check if all survey files exist before proceeding\n",
        "        if os.path.exists(panas_file) and os.path.exists(stai_file) and os.path.exists(dim_file):\n",
        "            # Read the survey CSV files\n",
        "            panas_df = pd.read_csv(panas_file)\n",
        "            stai_df = pd.read_csv(stai_file)\n",
        "            dim_df = pd.read_csv(dim_file)\n",
        "\n",
        "            # Merge survey data on CONDITION column\n",
        "            merged_df = panas_df.merge(stai_df, on='CONDITION').merge(dim_df, on='CONDITION')\n",
        "\n",
        "            # Rename 'CONDITION' column to 'Condition'\n",
        "            merged_df.rename(columns={'CONDITION': 'Condition'}, inplace=True)\n",
        "\n",
        "            # Add Patient_ID column\n",
        "            merged_df.insert(0, 'Patient_ID', patient_id)\n",
        "\n",
        "            # Merge with protocol data to add Start_Time and End_Time\n",
        "            patient_protocols = protocol_data[protocol_data['Patient_ID'] == patient_id]\n",
        "            merged_df = merged_df.merge(patient_protocols[['Condition', 'Start_Time', 'End_Time']],\n",
        "                                        on='Condition', how='left')\n",
        "\n",
        "            # Reorder columns: Move 'Condition', 'Start_Time', and 'End_Time' after 'Patient_ID'\n",
        "            cols = merged_df.columns.tolist()\n",
        "            for col in ['Condition', 'Start_Time', 'End_Time']:\n",
        "                cols.insert(1, cols.pop(cols.index(col)))\n",
        "            merged_df = merged_df[cols]\n",
        "\n",
        "            return merged_df\n",
        "        else:\n",
        "            print(f\"Missing one or more survey files for {patient_id}. Skipping merge.\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error merging survey data for {patient_id}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Process all patient survey data\n",
        "for file in os.listdir(output_folder_quest):\n",
        "    if file.endswith(\"_PANAS.csv\"):  # Identify patients by PANAS file existence\n",
        "        patient_id = file.split(\"_\")[0]  # Extract patient ID\n",
        "        merged_survey_data = merge_surveys(patient_id, output_folder_quest, protocol_data)\n",
        "        if merged_survey_data is not None:\n",
        "            all_survey_data.append(merged_survey_data)\n",
        "\n",
        "# Compile all patients' survey data into a single DataFrame and save to CSV\n",
        "if all_survey_data:\n",
        "    df_all_survey_data = pd.concat(all_survey_data, ignore_index=True)\n",
        "    df_all_survey_data.to_csv(output_merged_file, index=False)\n"
      ],
      "metadata": {
        "id": "sjOUkj0Oswgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Label compiled questionnaire data"
      ],
      "metadata": {
        "id": "KvfMo_0nuECW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the compiled survey data\n",
        "survey_file = \"/content/drive/MyDrive/Capstone/compiled_survey_data.csv\"\n",
        "df_survey = pd.read_csv(survey_file)\n",
        "\n",
        "# Define PANAS question groups\n",
        "orig_panas_pa = [\"PANAS_Active\", \"PANAS_Interested\", \"PANAS_Inspired\", \"PANAS_Strong\",\n",
        "                 \"PANAS_Excited\", \"PANAS_Proud\", \"PANAS_Enthusiastic\", \"PANAS_Alert\",\n",
        "                 \"PANAS_Determined\", \"PANAS_Attentive\"]\n",
        "\n",
        "orig_panas_na = [\"PANAS_Distressed\", \"PANAS_Annoyed\", \"PANAS_Strong\", \"PANAS_Guilty\",\n",
        "                 \"PANAS_Scared\", \"PANAS_Hostile\", \"PANAS_Irritable\", \"PANAS_Ashamed\",\n",
        "                 \"PANAS_Nervous\", \"PANAS_Jittery\", \"PANAS_Afraid\"]\n",
        "\n",
        "mod_panas_pa = orig_panas_pa + [\"PANAS_Happy\"]\n",
        "mod_panas_na = orig_panas_na + [\"PANAS_Stressed\", \"PANAS_Frustrated\", \"PANAS_Angry\", \"PANAS_Irritated\", \"PANAS_Sad\"]\n",
        "\n",
        "# Define STAI question groups (Reversed and Normal)\n",
        "stai_normal = [\"STAI_nervous\", \"STAI_jittery\", \"STAI_worried\"]\n",
        "stai_reversed = [\"STAI_ease\", \"STAI_relaxed\", \"STAI_pleasant\"]\n",
        "\n",
        "# Reverse STAI scores (1↔4, 2↔3)\n",
        "reverse_mapping = {1: 4, 2: 3, 3: 2, 4: 1}\n",
        "for col in stai_reversed:\n",
        "    df_survey[col] = df_survey[col].map(reverse_mapping)\n",
        "\n",
        "# Compute PANAS Scores\n",
        "df_survey[\"Orig_PANAS_PA\"] = df_survey[orig_panas_pa].sum(axis=1)\n",
        "df_survey[\"Orig_PANAS_NA\"] = df_survey[orig_panas_na].sum(axis=1)\n",
        "df_survey[\"Mod_PANAS_PA\"] = df_survey[mod_panas_pa].sum(axis=1)\n",
        "df_survey[\"Mod_PANAS_NA\"] = df_survey[mod_panas_na].sum(axis=1)\n",
        "\n",
        "# Self-Reported Stress Label\n",
        "df_survey[\"Stress_label\"] = df_survey[\"PANAS_Stressed\"] - 1  # Shift values to match (0 = no stress, 1 = slight stress, etc.)\n",
        "\n",
        "# Compute Modified STAI Score\n",
        "df_survey[\"Mod_STAI\"] = df_survey[stai_normal + stai_reversed].sum(axis=1)\n",
        "\n",
        "# SAM Questionnaire Labels\n",
        "df_survey[\"Valence_label\"] = df_survey[\"SAM_valence\"]\n",
        "df_survey[\"Arousal_label\"] = df_survey[\"SAM_arousal\"]\n",
        "\n",
        "# Save the updated dataset\n",
        "output_labeled_file = \"/content/drive/MyDrive/Capstone/labeled_survey_data.csv\"\n",
        "df_survey.to_csv(output_labeled_file, index=False)\n"
      ],
      "metadata": {
        "id": "B6Af7_sIuMAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Update questionnaire labels"
      ],
      "metadata": {
        "id": "2DMBseZxuMWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the compiled survey data\n",
        "survey_file = \"/content/drive/MyDrive/Capstone/compiled_survey_data.csv\"\n",
        "df_survey = pd.read_csv(survey_file)\n",
        "\n",
        "# Define PANAS question groups\n",
        "orig_panas_pa = [\"PANAS_Active\", \"PANAS_Interested\", \"PANAS_Inspired\", \"PANAS_Strong\",\n",
        "                 \"PANAS_Excited\", \"PANAS_Proud\", \"PANAS_Enthusiastic\", \"PANAS_Alert\",\n",
        "                 \"PANAS_Determined\", \"PANAS_Attentive\"]\n",
        "\n",
        "orig_panas_na = [\"PANAS_Distressed\", \"PANAS_Annoyed\", \"PANAS_Strong\", \"PANAS_Guilty\",\n",
        "                 \"PANAS_Scared\", \"PANAS_Hostile\", \"PANAS_Irritable\", \"PANAS_Ashamed\",\n",
        "                 \"PANAS_Nervous\", \"PANAS_Jittery\", \"PANAS_Afraid\"]\n",
        "\n",
        "# Define STAI question groups (Reversed and Normal)\n",
        "stai_normal = [\"STAI_nervous\", \"STAI_jittery\", \"STAI_worried\"]\n",
        "stai_reversed = [\"STAI_ease\", \"STAI_relaxed\", \"STAI_pleasant\"]\n",
        "\n",
        "# Reverse STAI scores (1↔4, 2↔3)\n",
        "reverse_mapping = {1: 4, 2: 3, 3: 2, 4: 1}\n",
        "for col in stai_reversed:\n",
        "    df_survey[col] = df_survey[col].map(reverse_mapping)\n",
        "\n",
        "# Compute PANAS Scores (Only original PANAS PA and NA)\n",
        "df_survey[\"PANAS_PA\"] = df_survey[orig_panas_pa].sum(axis=1)\n",
        "df_survey[\"PANAS_NA\"] = df_survey[orig_panas_na].sum(axis=1)\n",
        "\n",
        "# Map Stress label: 1 → 0 (no stress), 2 & 3 → 1 (low stress), 4 & 5 → 2 (high stress)\n",
        "df_survey[\"Stress_label\"] = df_survey[\"PANAS_Stressed\"].map({1: 0, 2: 1, 3: 1, 4: 2, 5: 2})\n",
        "\n",
        "# Compute STAI Score\n",
        "df_survey[\"Mod_STAI\"] = df_survey[stai_normal + stai_reversed].sum(axis=1)\n",
        "\n",
        "# Map Anxiety Level: 0 (no to low anxiety) = STAI 4-9, 1 (moderate) = STAI 10-12, 2 (High) = STAI 13-24\n",
        "df_survey[\"Anxiety_Level\"] = df_survey[\"Mod_STAI\"].apply(lambda x: 0 if x <= 9 else (1 if x <= 12 else 2))\n",
        "\n",
        "# Depression Label: 1 (depressed) if valence < 5 and arousal < 5, else 0\n",
        "df_survey[\"Depression_label\"] = ((df_survey[\"SAM_valence\"] < 5) & (df_survey[\"SAM_arousal\"] < 5)).astype(int)\n",
        "\n",
        "# Save the updated dataset\n",
        "output_labeled_file = \"/content/drive/MyDrive/Capstone/updated_labeled_survey_data.csv\"\n",
        "df_survey.to_csv(output_labeled_file, index=False)\n"
      ],
      "metadata": {
        "id": "q0-qUw75uQsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting EDA and HRV features in 60s (1 min) intervals for all patients"
      ],
      "metadata": {
        "id": "7c6i7iMmu8Qd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import neurokit2 as nk\n",
        "from scipy.stats import linregress\n",
        "\n",
        "# Paths\n",
        "data_folder = '/content/drive/MyDrive/Capstone/WESAD'\n",
        "protocol_file = '/content/drive/MyDrive/Capstone/output_quest_data/compiled_protocol_data.csv'\n",
        "output_file = '/content/drive/MyDrive/Capstone/eda_hrv_features_across_conditions_60s.csv'\n",
        "\n",
        "# Load protocol times data\n",
        "protocol_data = pd.read_csv(protocol_file)\n",
        "\n",
        "# Define time split window size (in seconds)\n",
        "split_window_size = 60\n",
        "fs = 700  # Sampling rate (Hz)\n",
        "\n",
        "# Initialize list to collect all features\n",
        "all_features = []\n",
        "\n",
        "# Function to process a single patient with ordered time-split windows\n",
        "def process_patient_time_splits(patient_id, data_folder, protocol_data):\n",
        "    pkl_path = os.path.join(data_folder, patient_id, f'{patient_id}.pkl')\n",
        "\n",
        "    # Load patient data\n",
        "    with open(pkl_path, 'rb') as f:\n",
        "        patient_data = pickle.load(f, encoding='latin1')\n",
        "\n",
        "    # Extract chest EDA and ECG signals\n",
        "    chest_EDA = patient_data['signal']['chest']['EDA'].flatten()\n",
        "    chest_ECG = patient_data['signal']['chest']['ECG'].flatten()\n",
        "    total_duration = len(chest_EDA) / fs  # Convert samples to seconds\n",
        "\n",
        "    # Filter protocol data for the specific patient\n",
        "    patient_protocols = protocol_data[protocol_data['Patient_ID'] == patient_id]\n",
        "\n",
        "    # Create a timeline for segment labeling\n",
        "    condition_labels = np.full(int(total_duration // split_window_size) + 1, \"Transition\", dtype=object)\n",
        "\n",
        "    # Assign condition labels based on protocol start and end times\n",
        "    for _, row in patient_protocols.iterrows():\n",
        "        condition = row['Condition']\n",
        "        start_time = row['Start_Time']\n",
        "        end_time = row['End_Time']\n",
        "\n",
        "        if pd.isnull(start_time) or pd.isnull(end_time):\n",
        "            continue\n",
        "\n",
        "        start_idx = int(start_time // split_window_size)\n",
        "        end_idx = int(end_time // split_window_size)\n",
        "\n",
        "        condition_labels[start_idx:end_idx + 1] = condition  # Assign condition label\n",
        "\n",
        "    patient_features = []\n",
        "\n",
        "    # Process time-split windows\n",
        "    num_samples_per_split = split_window_size * fs\n",
        "    num_splits = len(chest_EDA) // num_samples_per_split\n",
        "\n",
        "    for split_idx in range(num_splits):\n",
        "        start_split = split_idx * num_samples_per_split\n",
        "        end_split = start_split + num_samples_per_split\n",
        "\n",
        "        eda_segment = chest_EDA[start_split:end_split]\n",
        "        ecg_segment = chest_ECG[start_split:end_split]\n",
        "\n",
        "        if len(eda_segment) < num_samples_per_split or len(ecg_segment) < num_samples_per_split:\n",
        "            continue  # Skip incomplete segments\n",
        "\n",
        "        # Determine segment condition label\n",
        "        segment_condition = condition_labels[split_idx]\n",
        "\n",
        "        # Process EDA signal\n",
        "        eda_signals, eda_info = nk.eda_process(eda_segment, sampling_rate=fs)\n",
        "\n",
        "        # Extract EDA components\n",
        "        tonic_eda = eda_signals['EDA_Tonic']\n",
        "        scr_peaks = eda_signals['SCR_Peaks']\n",
        "\n",
        "        # Compute statistics for EDA components\n",
        "        eda_features = {\n",
        "            'SCL_Mean': np.mean(tonic_eda),  # Skin Conductance Level (SCL)\n",
        "            'SCL_STD': np.std(tonic_eda),\n",
        "            'SCR_Peaks_Count': len(scr_peaks.dropna()),  # Number of SCR peaks\n",
        "            'EDA_Raw_Mean': np.mean(eda_segment),\n",
        "            'EDA_Raw_STD': np.std(eda_segment),\n",
        "            'EDA_Raw_Dynamic_Range': np.max(eda_segment) - np.min(eda_segment)\n",
        "        }\n",
        "\n",
        "        # Process ECG signal\n",
        "        ecg_cleaned = nk.ecg_clean(ecg_segment, sampling_rate=fs)\n",
        "        peaks, info = nk.ecg_peaks(ecg_cleaned, sampling_rate=fs, correct_artifacts=True)\n",
        "\n",
        "        # Extract ECG statistics\n",
        "        ecg_features = {\n",
        "            'ECG_Mean': np.mean(ecg_segment),\n",
        "            'ECG_Min': np.min(ecg_segment),\n",
        "            'ECG_Max': np.max(ecg_segment),\n",
        "            'ECG_Median': np.median(ecg_segment),\n",
        "            'ECG_STD': np.std(ecg_segment)\n",
        "        }\n",
        "\n",
        "        # Extract HRV features if enough R-peaks are detected\n",
        "        if len(peaks['ECG_R_Peaks']) >= 2:\n",
        "            hrv_metrics = nk.hrv(peaks, sampling_rate=fs, show=False)\n",
        "\n",
        "            # Extract required HRV features\n",
        "            hrv_features = {\n",
        "                'HRV_RMSSD': hrv_metrics['HRV_RMSSD'].iloc[0] if 'HRV_RMSSD' in hrv_metrics else np.nan,\n",
        "                'HRV_SDNN': hrv_metrics['HRV_SDNN'].iloc[0] if 'HRV_SDNN' in hrv_metrics else np.nan,\n",
        "                'HRV_pNN50': hrv_metrics['HRV_pNN50'].iloc[0] if 'HRV_pNN50' in hrv_metrics else np.nan,\n",
        "                'HRV_SDANN': hrv_metrics['HRV_SDANN'].iloc[0] if 'HRV_SDANN' in hrv_metrics else np.nan,\n",
        "                'HRV_MeanRR': hrv_metrics['HRV_MeanNN'].iloc[0] if 'HRV_MeanNN' in hrv_metrics else np.nan,\n",
        "                'HRV_MedianRR': hrv_metrics['HRV_MedianNN'].iloc[0] if 'HRV_MedianNN' in hrv_metrics else np.nan,\n",
        "                'HRV_HF': hrv_metrics['HRV_HF'].iloc[0] if 'HRV_HF' in hrv_metrics else np.nan,\n",
        "                'HRV_LF': hrv_metrics['HRV_LF'].iloc[0] if 'HRV_LF' in hrv_metrics else np.nan,\n",
        "                'HRV_LF_HF_Ratio': hrv_metrics['HRV_LFHF'].iloc[0] if 'HRV_LFHF' in hrv_metrics else np.nan,\n",
        "                'HRV_SampEN': hrv_metrics['HRV_SampEn'].iloc[0] if 'HRV_SampEn' in hrv_metrics else np.nan\n",
        "            }\n",
        "        else:\n",
        "            hrv_features = {key: np.nan for key in [\n",
        "                'HRV_RMSSD', 'HRV_SDNN', 'HRV_pNN50', 'HRV_SDANN',\n",
        "                'HRV_MeanRR', 'HRV_MedianRR', 'HRV_HF', 'HRV_LF',\n",
        "                'HRV_LF_HF_Ratio', 'HRV_SampEN'\n",
        "            ]}\n",
        "\n",
        "        # Append extracted features\n",
        "        patient_features.append({\n",
        "            'Patient_ID': patient_id,\n",
        "            'Condition': segment_condition,  # Assigned condition label (or \"Transition\")\n",
        "            'Time_Split': split_idx + 1,\n",
        "            **eda_features,\n",
        "            **ecg_features,\n",
        "            **hrv_features\n",
        "        })\n",
        "\n",
        "    return patient_features\n",
        "\n",
        "# Loop through all patient folders and process\n",
        "for patient_folder in os.listdir(data_folder):\n",
        "    patient_id = patient_folder  # Patient ID corresponds to folder name (e.g., 'S2')\n",
        "    patient_path = os.path.join(data_folder, patient_folder)\n",
        "\n",
        "    if os.path.isdir(patient_path) and os.path.exists(os.path.join(patient_path, f'{patient_id}.pkl')):\n",
        "        patient_features = process_patient_time_splits(patient_id, data_folder, protocol_data)\n",
        "        all_features.extend(patient_features)\n",
        "\n",
        "# Convert all features to a DataFrame and save to CSV\n",
        "df_all_features = pd.DataFrame(all_features)\n",
        "df_all_features.to_csv(output_file, index=False)"
      ],
      "metadata": {
        "id": "fKjfCZ7MwXd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Labeling Extracted Features"
      ],
      "metadata": {
        "id": "iC_AtDl8wicw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "patient_features_60s = pd.read_csv(\"/content/drive/MyDrive/Capstone/eda_hrv_features_across_conditions_60s.csv\")\n",
        "df_labels = pd.read_csv(\"/content/drive/MyDrive/Capstone/updated_labeled_survey_data.csv\")\n",
        "df_labeled_patient_features_60s = patient_features_60s.merge(df_labels, on=[\"Patient_ID\", \"Condition\"], how=\"left\")\n",
        "df_labeled_patient_features_60s.to_csv(\"/content/drive/MyDrive/Capstone/labeled_patient_eda_hrv_features_60s.csv\", index=False)"
      ],
      "metadata": {
        "id": "6c6ECLnRwnG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DWFDsUXAxD1f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}